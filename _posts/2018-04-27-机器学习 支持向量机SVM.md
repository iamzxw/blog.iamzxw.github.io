---
layout:     post   				    # 使用的布局（不需要改）
title:      支持向量机SVM				# 标题 
subtitle:      #副标题
date:       2018-04-27 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/mycode.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - 核方法
    - 线性可分
---

> 



SVM最初是为了解决二分类问题，后面又推广到多分类、回归、异常点检测等问题。

书本上大部分只重点讲述SVM在二分类问题上的应用。

比如李航老师的《统计学习方法》中有这个一句话：

> SVM是定义在特征空间上的间隔最大的线性分类器，学习策略是间隔最大化，最终可以转化为带约束的最优问题求解。

这里面包含三个信息点：特征空间、间隔最大、线性分类器。

先看特征空间和线性分类器这两个点，定义在特征空间上的线性分类器。

也就是说，无论如何，SVM是一个线性分类器，这一点是不变的，即使有非线性SVM。不同的是，这个特征空间可以变。如果给定的数据集在原始数据的特征空间中线性可分，或者勉强线性可分，SVM就是定义在原始特征空间的线性分类器。如果给定数据集T在原始数据的特征空间中线性线性不可分，就需要通过核方法将原始空间映射到高维希尔伯特空间，使得数据线性可分，然后再学习一个线性分类器SVM。这里面涉及到三种类型的数据集，线性可、线性不可分数据集、非线性数据集，分别对应三种支持向量机。

再看间隔最大，正好对应线性可分支持向量机。

间隔的直观理解就是样本点到决策面的最短距离。

当训练数据集线性可分时，存在无穷个分离超平面可以将数据正确分类。间隔最大，就意味着SVM使用间隔最大这种学习策略，从无穷个分离超平面中找到一个使得的间隔最大的超平面，满足间隔最大的超平面只有一个。这意味着SVM的解是唯一的（不同于神经网络利用误分类最小的策略，求得）

根据间隔最大化策略，SVM可以形式化为一个带约束的最优化问题。
在给定约束条件下，最大化几何间隔。这里的约束指的是，所有数据集中的样板点到决策面的间隔一定会大于等于某个值。在这个条件下，最大化距离间隔。使得等号成立的点就是支持向量。
最大化几何间隔可以转化为最小化决策面法向量的平方||w||，构造并求解这个约束优化问题。

因为是带约束的最优化问题，也就是凸最优化问题，可以借助最优化理论求解。引入拉格朗日函数，将原始问题转化为极大极小问题，L（w，b，α）。先求L（w，b，α）对w，b的极小，在求对α的极大。 这就得到了线性可分SVM的对偶形式算法，构造并求解对偶形式的约束优化问题。


直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类，不仅将正负实例点分开，而且对最难分的实例点，离超平面最近的点，也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例，有很好的分类预测能力。

线性不可分数据，或者大部分数据线性可分。也就是说某些样本点不满足间隔大于等于某个值的约束条件。为每个样本点引入一个松弛变量，使得函数间隔加上松弛变量就满足约束条件。目标函数变为 。前者表示间隔尽可能大，后者使误分类点的个数尽可能小。C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小。

总结来说，对于线性支持向量机学习来说，其模型为分离超平面wx + b = 0及决策函数，学习策略为软间隔最大化，学习算法为凸二次规划。

还有另一种解释是，最小化带有正则项的合页损失函数
当样本点被正确分类且函数间隔y(wx + b)大于1时，损失是0，否则损失是1 - y(wx + b)。



非线性分类问题。只能通过非线性模型才能很好地进行分类的问题。

给定核函数K，特征空间H和映射函数的取法不唯一。
核函数K是定义在原始特征空间的函数，原始空间任意两个变量x，z，
线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积，可以非常方便引入核函数。σ