---
layout:     post
title:      深度神经网络的加速与压缩 / 学习笔记
subtitle:   #副标题
date:       2019-7-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 模型加速
    - 模型剪切
    - Low-Rank 低秩分解
    - 
    - L2范式
---

常见方法：
- 低秩分解
- 





### 低秩分解

由于卷积神经网络中的主要计算量在于卷积计算，而卷积计算本质上是矩阵分析的问题。

通过SVD奇异值分解等矩阵分析方法可以有效减少矩阵运算的计算量。常见的Tensor分解方法有CP分解、Tucker分解、Tensor Train分解和Block Term分解等等。

因为矩阵分析相关研究已经很成熟了，所以基于矩阵分解的模型加速与压缩方法经过过去的发展也已经基本成熟。

- 矩阵分解方法显而易见、比较容易实现，所以该研究的都已经研究了
- 现在越来越多网络中采用1×1的卷积，而这种小的卷积使用矩阵分解的方法很难实现网络加速和压缩。




### 剪枝Pruning

最早的有Deep Compression随机剪枝方法，后来的结构化Pruning、卷积核Pruning、梯度Pruning等。



### 量化Quantization

[](https://www.jiqizhixin.com/articles/2018-05-18-4)