---
layout:     post   				    # 使用的布局（不需要改）
title:     	2018 之江杯全球人工智能大赛 零样本图像识别 赛后总结
subtitle:      #副标题
date:       2018-11-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Zero-shot learning
    - 图像分类
    - 天池
---

___
### 竞赛规则陈述
- 将其视为 ZSL 问题，而非 gZSL 问题(即 testD 中是不存在 trainD 中的 class)
- 假设 test 中的数据完全不参与任何训练过程，不考虑任何 transductive 方法

___
### 整体思路
在此次比赛中，通过参考论文[2]，使用 GCN (Graph Convolutional Network)来实现
从 word Embedding 到 image feat 的映射，从而为每个 test class 预测出所对应的
image feat。此外，我们比较了 GCN 与直接映射(DEM 参考论文[3])在 testD 上的性
能。在其他条件完全相同的情况下，GCN 达到的最大 ACC 为 47% 而 DEM 最大 ACC 为
29%。

___
### 整体框架
- 使用 ABCD 训练集全部图片训练 CNN 模型分类模型，详细见 CNN 分类模型
- 在训练后的 CNN 模型上提取 train 与 test 每张图片所对应的 image_feat.
- 通过 train_class_word_Embedding 与 train_image_feat 训练 GCN model ，详细见 GCN 模型
- 在训练后的 GCN 模型上预测 test_image_feature ，详细见 test 特征预测
- 预测：通过计算 test_image_feat 与每个 test_image_feature 的余弦相似度，将余弦相似度最大的 test_image_feature 所对应的 label 视为此张图片的预测 label

___
### CNN 分类模型
- 训练数据集：Dataset ABCD train 中包含的全部 365 个类别图片
- validation 划分：随机将全部图片按照 9：1 划分，前者用于训练 CNN model
- CNN 模型：通过尝试 DenseNet，ResNet，InceptionResNetV2，SENet 等多种模型，最终选择 InceptionResNetV2，并用其提取每张图片的 imageFeat
- 为了提高模型提取 feature 的性能，采用如下技巧： o data argument
* mixup[1]
* random erasing
- 模型训练结果：当训练 60 个 epoch 之后，validation acc 不在上升，稳定在
72%。

### Image Feat 特征提取
- 用训练后 CNN 模型为 data set D 中的 train 与 test 中的每张图片分别提取 image feat
- 每张图片的 image feat 即为 InceptionResNetV2 中的最后一个 FC 层，对应维度
为 1024


### GCN 模型
- 模型结构：GCN 的输入为每个 class 所对应的 word embeding，输出为每个 class 所对应的 image Feature，即通过 GCN 网络训练一个从 word embeding 到 image feature 的映射。


- Word Embeding：word embeding 为每个 class 所对应的词向量
- Image Feat：在训练时，train class 所对应的 image feat 为每个类别的全部图片。因为没有 test 所对应的 image feat，所以在计算 loss 时忽略 test。
- 视觉关系矩阵 A：其中关系矩阵 A 为一个 0 1 矩阵。A 的维度为 225*225， 因为data D 中 trian 与 test 一共 225 类。若两个类别在视觉上相似，我们即认为两个 class 之间有连接，在矩阵 A 中所对应的位置为 1，反之为 0. 那么如何确定两个类别之间是否相似？对于 train 中的 class，我们通过计算 class 在 image feat 空间中的余弦相似度距离来确定，对于 test 中的 class，我们主要通过属性中的形状以及颜色来确定。在试验中，我们为每个 class 选择两个相近的类。
- 与 dem 模型的对比: 我们会发现 GCN 与 dem 最主要的差别在于 GCN 中存在一个视觉关系矩阵 A。试验中，在 image feat 相同的情况下，我们分别验证了 dem model 和 GCN model 的性能。DEM 在 testD 上的 acc 为 29% GCN 在 testD 上的 acc 为 47%。之所以 GCN 会取得更好的效果在于，GCN 模型不仅仅获取了 word embeding 上的语义信息，而且还通过视觉关系矩阵 A 获取了视觉上相似的
信息。 

### test 特征提取 以及 label 预测
- 通过训练好的 GCN 模型，通过 test class 多对应的 word embedding 可以为每个
test class 预测出一个 image feature。将其称作 test image feature 池。
- 在 label 预测阶段，我们首先得到每张图片通过 CNN 提取出的 image feat，然后
将其与 test image feature 池中的每个 image feature 计算余弦相似度，余弦相似
度最大的 image feature 视为此张图片的 label

### Word Embedding 获取
- Wordnet 中包含大量丰富的物体类别，我们通过物体类别在 Wordnet 中的 wnid，找到类别对应的名称。比如，数据集中有两个 mouse 类别，一个指动物，一个指电脑设备。如果只使用 mouse 一个单词来换取词向量，在语义空间很难区分两者。我们通过 wordnet 的层级结构，在词向量中加入类别的若干个父节点信息。详细见 synset2vec_hier.py 代码。论文参考[4]
- 相较于官方给出的参考 word embedding，我们自己获取的词向量在 ZSL 识别正确率上有很大提升。


### 模型创新点：
- 通过 Wordnet 中物体类别的 wnid 获取其若干父节点，并将父节点的类别词向量以一定权重比例加入到当前类别词向量中，使得我们获取的词向量不仅具有类别语义信息，还有类别间的层级结构(Hierarchy)信息
- 通过 Graph Convolutional Network 实现从 word embedding 到 image Feature 映射。将视觉信息(视觉关系矩阵)与语义信息(word embeding)结合，明显提高了映射效果。
- 通过增加 mixup 与 random erasing 来提高 CNN 模型提取特征能力

### 结果迭代时间：
- CNN 模型共迭代 60epoch，约 60 小时，两块 P100GPU
- GCNmodel 迭代 15-20epoch，约 5 分钟

> 参考论文：  
> [1] mixup: BEYOND EMPIRICAL RISK MINIMIZATION  
> [2] Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs  
> [3] Learning a Deep Embedding Model for Zero-Shot Learning  
> [4] Zero-shot Image Tagging by Hierarchical Semantic Embedding  

### 比赛感悟：
比赛规定，可以借助外部属性知识库，甚至选手可以自己微调属性值。但是人工标注属性值，有很大的主观性，所以我们在借用属性语义时很谨慎。尽量不去大量改动属性值，或者借用外部属性，或者自动抽取类别属性。只是借助类别属性构造测试集类别与训练集类别的视觉相似关系。类别属性的质量直接影响到模型预测准确率，而属性标注又是一件很主观的事。我们认为，选手可以借助外部属性知识库这项规定有点模糊。这会导致竞赛朝向获取有效类别属性这个单一方向发展，而没有焦距到 ZSR 模型创新，或者 ZSR 实质问题。