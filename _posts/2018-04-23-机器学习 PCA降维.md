---
layout:     post   				    # 使用的布局（不需要改）
title:      机器学习 / PCA降维
subtitle:      #副标题
date:       2018-04-23 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 降维
    - 数据可视化
---

首先PCA是一种线性降维方法。

在高维度空间中，有数据样本稀疏、距离计算困难等问题，是所有机器学习方法都会面临的问题，可以通过降维的方式来缓解。

一般来说，想获得低维子空间，最简单的方法就是对原始高维空间进行线性变换。原始空间的样本乘以一个变换矩阵，就可以了。Z = WX；对变换矩阵W施加不同的约束，会得到不同的线性降维方法。

PCA算法要求低维子空间对样本具有最大可分性，就是样本点在超平面上的投影尽可能分开，则需最大化投影点的方差。这就得到了PCA降维的带约束优化目标函数，通过拉格朗日乘子法求解。

只需要对X进行奇异值分解，或者对协方差矩阵进行特征值分解，得到投影矩阵。

PCA仅需保留W与样本的均值向量，即可通过简单的向量剪发和矩阵-向量乘法将新样本投影到低维空间。


核化线性降维

当数据集不同维度上的方差分布不均匀的时候，PCA最有用。（如果是一个球壳形数据集，PCA不能有效的发挥作用，因为各个方向上的方差都相等；没有丢失大量的信息维度一个都不能忽略）


为什么能进行降维？ 在很多时候，我们观察或收集到的数据样本虽然是高维的，但是与学习任务密切相关的也许仅是某个低维分布。 


PCA算法描述：
输入：样本集D = {x}，低维空间维数d’
过程：
对所有样本点进行中心化：x = x - 均值
计算样本的协方差矩阵XXT
对协方差矩阵XXT做特征值分解
取最大的d’个特征值对应的特征向量。
输出：投影变换矩阵W 

1.对所有样本点进行中心化，x=x - 均值
2.计算数据的主成分：矩阵的主成分是协方差矩阵的特征向量按照对应的特征值大小排序得到。
(1)计算数据协方差矩阵，对协方差矩阵做特征值分解
(2)用数据矩阵的奇异值分解来找协方差矩阵的特征向量和特征值的平方根

第一主成分是最大特征值对应的特征向量；

PCA的应用：高维数据可视化，