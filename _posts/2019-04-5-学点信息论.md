---
layout:     post
title:      学点信息论
subtitle:   #副标题
date:       2019-4-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 交叉熵
    - DenseNet
    - 
---

概括：
- 随机变量X的熵：H(X) = -ΣP(x)logP(x) x∈X

- 相关信息Y的出现，降低X的不确定性，条件熵：H(X|Y) = -ΣP(x,y)logP(x|y)  x∈X, y∈Y

- 如何量化两个随机变量X，Y的相关性, 互信息：I(X；Y) = H(X) - H(X|Y) = ΣP(x,y)logP(x,y)/P(x)P(y) x∈X, y∈Y

- 如何量化两个函数f(x),g(x)的相关性，相对熵，交叉熵，KL散度，KL(f(x) || g(x)) = Σf(x)logf(x)/g(x) x∈X


### 信息熵

一条信息的信息量和它的不确定性有着直接的关系。信息量就等于不确定性的多少。

香农使用比特Bit来度量信息量

32个队伍等概率情况下, 不确定性最大
H = log(32) = 5

32个队伍不等概率情况下，根据先验知识，已经给出了写信息，所以不确定性相对小些
H = -(p2.log(p2) + p2.log(p2) + ... + p32.log(p32)) < 5

极端例子，其中一个队伍概率为0.999

H = -(1.log1 + 0.log0 + 0.log0) 逼近0

熵的定义：对于任意随机变量X

H(X) = -ΣP(x)logP(x) x∈X

随机变量X的不确定性越大，极端情况为均匀概率，熵就越大，把它搞清楚所需要的信息量也就越大。

> 一本50万字的中文书平均有多少信息量？
> 常用的汉字有7000个，假如每个字等概率出现，大约需要log7000 = 13个比特表示一个汉字，考虑到汉字使用不平衡，上下文相关性等，每个汉字的信息熵也就5比特。所以一本50万字的中文书，信息量大约是250万比特

不同语言的冗余度差距很大，而汉语在所有语言中冗余度是相对小的。一本英文书，翻译成中文，书本会薄很多。

信息是消除系统不确定性的唯一办法。

网页搜索本质上是利用信息（用户提供，历史记录等）消除不确定性的过程。 为什么这些相关信息能够消除不确定性？

### 条件熵


在给定随机变量Y的条件下，随机变量X的条件熵为：

H(X|Y) = -ΣP(x,y)logP(x|y)  x∈X, y∈Y

联合概率P(x,y)表示x,y同时出现的概率，条件概率p(x|y)表示y出现的情况下，x出现的概率

H(X) >= H(X|Y)表明给出Y的信息，关于X的不确定性就减少了。减少了多少呢？

如果把Y看成前一个字，在数学上证明二元模型的不确定性小于一元模型。

扩展：
H(X|Y,Z) = -ΣP(x,y,x)logP(x|y,z)  x∈X, y∈Y, z∈Z

H(X|Y) >= H(X|Y,Z): 三元模型应该比二元好

当我们提供的相关信息Y，X和随机变量X一点关系也没有，等号成立。或者相关信息本身一点信息量都没有。


### 互信息

当获取的信息和要研究的事物相关时，这些信息才能帮助我们消除不确定。如何量化“相关性”。

I(X;Y) = ΣP(x,y)logP(x,y)/P(x)P(y) x∈X, y∈Y

可以证明 I(X；Y) = H(X) - H(X|Y)

在自然语言处理中，两个随机变量的互信息很容易计算。只要有足够的语料库，就不难估计出互信息公式中的P(X,Y)，P(X), P(Y)的概率。

### 相对熵，交叉熵，KL散度(Kullback-Lerbler Divergence)

和衡量两个变量相关性的互信息不同，相对熵用来衡量两个取值为正数的函数的相似性。

KL(f(x) || g(x)) = Σf(x)logf(g)/g(x) x∈X

f(x) g(x)定义在同一变量空间X上的函数

对两个完全相同的函数，它们的相对熵等于零

对于概率分布或者概率密度函数，取值都大于零，相对熵可以度量两个随机分布的差异性。

利用相对熵，可以得到信息检索中最重要 一个概念：词频率-逆向文本频率TF-IDF