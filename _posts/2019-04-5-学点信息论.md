---
layout:     post
title:      学点信息论
subtitle:   #副标题
date:       2019-4-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 交叉熵
    - 信息熵
    - 互信息
    - 条件熵
    - KL散度
---

概括：
- 随机变量X的熵：`H(X) = -ΣP(x)logP(x) x∈X`

- 相关信息Y的出现，降低X的不确定性，条件熵：`H(X|Y) = -ΣP(x,y)logP(x|y)  x∈X, y∈Y`

- 如何量化两个随机变量X，Y的相关性, 互信息：`I(X；Y) = H(X) - H(X|Y) = ΣP(x,y)logP(x,y)/P(x)P(y) x∈X, y∈Y`

- 如何量化两个函数f(x),g(x)的相关性，相对熵，交叉熵，KL散度: `KL(f(x) || g(x)) = Σf(x)logf(x)/g(x) x∈X`


### 信息熵

一条信息的信息量和它的不确定性有着直接的关系。信息量就等于不确定性的多少。

香农使用比特Bit来度量信息量

32个队伍等概率情况下, 不确定性最大

`H = log(32) = 5`

32个队伍不等概率情况下，根据先验知识，已经给出了写信息，所以不确定性相对小些

`H = -(p2.log(p2) + p2.log(p2) + ... + p32.log(p32)) < 5`

极端例子，其中一个队伍概率为0.999

`H = -(1.log1 + 0.log0 + 0.log0) 逼近0`

熵的定义：对于任意随机变量X

`H(P) = Σ P(x) log 1/P(x) x∈X`

随机变量X的不确定性越大，极端情况为均匀概率，熵就越大，把它搞清楚所需要的信息量也就越大。

> 一本50万字的中文书平均有多少信息量？
> 常用的汉字有7000个，假如每个字等概率出现，大约需要log7000 = 13个比特表示一个汉字，考虑到汉字使用不平衡，上下文相关性等，每个汉字的信息熵也就5比特。所以一本50万字的中文书，信息量大约是250万比特

不同语言的冗余度差距很大，而汉语在所有语言中冗余度是相对小的。一本英文书，翻译成中文，书本会薄很多。

信息是消除系统不确定性的唯一办法。

网页搜索本质上是利用信息（用户提供，历史记录等）消除不确定性的过程。 为什么这些相关信息能够消除不确定性？

**Note:**

**信息量/自信息selft-information的定义是：单个事件(某个球队)的信息量 `I(x) = log 1/P(x)`。而信息熵是对整个随机变量X概率分布的量化, `H(X) = E(I) = Σ P(x) log 1/P(x)`， 也就是说熵的本质是单个事件自信息的期望，**

信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的【最小努力】（猜题次数、编码长度等）的大小就是信息熵

根据系统的真实分布制定最优策略去消除系统的不确定性时，我们所付出的努力是最小的

当我们使用非最优策略消除系统的不确定性，所需要付出的努力的大小我们该如何去衡量呢？ 交叉熵


### 交叉熵

衡量在给定的真实分布P(X)下，使用非真实分布Q(X)所指定的策略消除系统的不确定性所需要付出的努力的大小

`H(P, Q) = Σ P(x) log 1/Q(x)`

**交叉熵越低，这个策略就越好，最低的交叉熵也就是使用了真实分布所计算出来的信息熵**

这也是为什么在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。


如何去衡量不同策略之间的差异呢？这就需要用到相对熵


### 相对熵，KL散度(Kullback-Lerbler Divergence)

用来衡量两个取值为正的函数或概率分布之间的差异, 相对熵 = 某个策略的交叉熵 - 信息熵

和衡量两个变量相关性的互信息不同，相对熵用来衡量两个取值为正数的函数的相似性。

`KL(f(x) || g(x)) =Σ f(x) log 1/g(x) - Σ f(x) log 1/f(x) =  Σf(x)logf(g)/g(x) x∈X`

f(x) g(x)定义在同一变量空间X上的函数

对两个完全相同的函数，它们的相对熵等于零

对于概率分布或者概率密度函数，取值都大于零，相对熵可以度量两个随机分布的差异性。

利用相对熵，可以得到信息检索中最重要 一个概念：词频率-逆向文本频率TF-IDF，词频在整个语料库的分布与词频在具体文档中分布之间的差异性

几个用途：
- 衡量两个概率分布的差异
- 衡量利用概率分布Q 拟合概率分布P 时的能量损耗
- KL散度的值始终大于0，并且当且仅当两分布相同时，KL散度等于0.

Note:
- KL散度不对称，即P到Q的距离，不等于Q到P的距离
- KL散度不满足三角距离公式，两边之和大于第三边，两边之差小于第三边

### 条件熵


在给定随机变量Y的条件下，随机变量X的条件熵为：

`H(X|Y) = -ΣP(x,y)logP(x|y)  x∈X, y∈Y`

联合概率`P(x,y)`表示x,y同时出现的概率，条件概率`p(x|y)`表示y出现的情况下，x出现的概率

`H(X) >= H(X|Y)`表明给出Y的信息，关于X的不确定性就减少了。减少了多少呢？

如果把Y看成前一个字，在数学上证明二元模型的不确定性小于一元模型。

扩展：
`H(X|Y,Z) = -ΣP(x,y,x)logP(x|y,z)  x∈X, y∈Y, z∈Z`

`H(X|Y) >= H(X|Y,Z)`: 三元模型应该比二元好

当我们提供的相关信息Y，X和随机变量X一点关系也没有，等号成立。或者相关信息本身一点信息量都没有。


### 互信息

当获取的信息和要研究的事物相关时，这些信息才能帮助我们消除不确定。如何量化“相关性”。

`I(X;Y) = ΣP(x,y)logP(x,y)/P(x)P(y) x∈X, y∈Y`

可以证明 `I(X；Y) = H(X) - H(X|Y)`

在自然语言处理中，两个随机变量的互信息很容易计算。只要有足够的语料库，就不难估计出互信息公式中的P(X,Y)，P(X), P(Y)的概率。



### 词频率-逆向文本频率TF-IDF

词频率Term Frequency: 关键词出现的次数除以网页的总字数。

搜索关键词和网页额相关性：TF1 + TF2 + ... + TFn


逆文本频率: logD/Dw, D是全部网页个数，Dw是包含关键词的网页个数

停止词会出现在所有网页中，所以logD/Dw = 0, 所有网页中，带有’原子能‘关键词的网页个数Dw相对少一些，logD/Dw就会相对大一些。


给关键词加上逆文本频率权重：TF1.IDF1 + TF2.IDF2 + ... + TFn.IDFn

- 停止词权重为零
- 关键词越是专业，包含关键词的网页数相对少一些，逆文本频率就会相对大一些，给该关键词的权重就很很大。

从信息论角度理解逆文本频率IDF的概念：就是在特定条件下关键词的概率分布的KL散度。

用关键词w的信息量作为它的权重：I(w) = -P(w)logP(w) = -TF(w)/N

可以得到TF-IDF与该关键词的信息量I(w)之间的一个等式

### 交叉熵损失函数
二分类情况：
L = -[( y log o + (1-y)log(1-o))]
  = y log 1/o + (1-y) log 1/(1-o)
  = 交叉熵
 
L = Σ y log p = -log p


nn.LogSoftmax

nn.NLLLoss

torch.nn.CrossEntropyLoss


### 交叉熵与均方差损失函数

均方差损失函数有个问题：如果误差越大，参数调整的幅度可能更小，训练更缓慢。网络参数w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数。

而使用交叉熵损失函数，当误差越大，参数梯度就越大，参数w调整得越快，训练速度也就越快。参数梯度和激活函数梯度没有关系。

当训练结果接近真实值时会因为梯度算子极小，使得模型的收敛速度变得非常慢。而由于交叉熵损失函数为对数函数，在接近上边界的时候，其仍然可以保持在高梯度状态，因此模型的收敛速度不会受到影响。

所以在回归任务中，比如预测image feature时，DEM训练很缓慢。应该就是这个原因。

[](https://blog.csdn.net/zb1165048017/article/details/48937135)
[知乎](https://www.zhihu.com/question/41252833)