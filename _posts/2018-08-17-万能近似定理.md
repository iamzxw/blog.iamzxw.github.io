---
layout:     post   				    # 使用的布局（不需要改）
title:      为什么神经网络能以任意精度拟合任意复杂度的函数？ 				# 标题 
subtitle:      #副标题
date:       2018-06-27 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/mycode.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Linear Regression
    - Logistic Regression
    - Universal approximation theorem
---

# 为什么神经网络能以任意精度拟合任意复杂度的函数？

在开始之前，我们先来看一下维基百科给出的万能近似定理(Universal approximation theorem)描述：
> In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of R, under mild assumptions on the activation function. 

Universal approximation theorem(Hornik et al., 1989;Cybenko, 1989)定理表明：前馈神经网络，只需具备单层隐含层和有限个神经单元，就能以任意精度拟合任意复杂度的函数。这是个已经被证明的定理。下面我们用一种轻松的方式解释，为什么神经网络（理论上）可以拟合任何函数？

## 1. 线性回归（Linear Regression）: 一道小学题


看过《神偷奶爸》这部电影的同学都知道，小黄人(Minions)非常喜欢吃香蕉。不过，现在它只有12个苹果，但它对苹果不感兴趣，想着如果谁能用香蕉换它的苹果就好了。不经意间，它发现了一个神奇的小屋。

