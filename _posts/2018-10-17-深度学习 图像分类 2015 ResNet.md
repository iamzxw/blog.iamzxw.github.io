---
layout:     post   				    # 使用的布局（不需要改）
title:     	深度学习 / 图像分类 / ResNet 论文笔记 
subtitle:      #副标题
date:       2018-9-19 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - ResNet
    - 图像分类
---

[Paper](https://arxiv.org/pdf/1512.03385.pdf)
[Code](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)

> 2015年，何凯明等人在一篇名为[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)的论文中首次提出ResNet。在当年ImageNet比赛中的classification任务获得第一名，获评CVPR2016最佳论文。因为它“简单与实用”并存，之后许多目标检测、图像分类任务都是建立在ResNet的基础上完成的，成为计算机视觉领域重要的基石结构。

___
### 退化问题（degradation problem）？

ResNet 诞生于一个美丽而简单的观察：为什么非常深度的网络在增加更多层时会表现得更差？直觉上推测，更深度的网络不会比更浅度的同类型网络表现更差。

当传统神经网络的层数从20增加为56时，网络的训练误差和测试误差均出现了明显的增长，也就是说，网络的性能随着深度的增加出现了明显的退化。 作者提出，这可能是因为更深的网络会伴随梯度消失,爆炸问题，从而阻碍网络的收敛。作者将这种加深网络深度但网络性能却下降的现象称为退化问题（degradation problem）。 ResNet就是为了解决这种退化问题而诞生的。

![](/img/cnn/resnet_1.PNG)

___
### ResNet怎么解决网络退化问题

> Batch normalization等方法，已经一定程度上缓解了这个问题，但依然不足以满足需求。

作者想到了构建恒等映射（Identity mapping）来解决这个问题，问题解决的标志是：增加网络层数，但训练误差不增加。
那怎么构建恒等映射呢？简单地说，原先的网络输入x，希望输出H(x)。现在我们改一改，我们令H(x)=F(x)+x，那么我们的网络就只需要学习输出一个残差F(x)=H(x)-x。作者提出，学习残差F(x)=H(x)-x会比直接学习原始特征H(x)简单的多。

___
### 残差网络组件

ResNet中有两种残差网络组件：basicBlock 和bottleNeck

![](/img/cnn/resnet_bottleneck_2.PNG)

#### BasicBlock

![](/img/cnn/resnet_basicblock.PNG)

输入数据分成两条路，一条路经过两个3x3卷积，另一条路直接短接，二者相加经过relu输出，十分简单。
主要提一个点：downsample，它的作用是对输入特征图大小进行减半处理，每个stage都有且只有一个downsample。后面我们再详细介绍

```python
class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(BasicBlock, self).__init__()
        # if norm_layer is None:
        #     norm_layer = nn.BatchNorm2d
        # if groups != 1 or base_width != 64:
        #     raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        # if dilation > 1:
        #     raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        # 第一个卷积
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        # 第二个卷积
        out = self.conv2(out)
        out = self.bn2(out)

        # 它的作用是对输入特征图大小进行减半处理
        if self.downsample is not None:
            identity = self.downsample(x)

        # 第二个卷积输出前加入上卷积的输入
        out += identity 
        out = self.relu(out)

        return out

```
![](/img/cnn/resnet_basicblock_2.PNG)


#### Bottleneck

![](/img/cnn/resnet_bottleneck_2.PNG)

Bottleneck先通过一个1x1的卷积减少通道数，使得中间卷积的通道数减少为1/4；中间的普通卷积做完卷积后输出通道数等于输入通道数；第三个卷积用于增加（恢复）通道数，使得bottleneck的输出通道数等于bottleneck的输入通道数。**这两个1x1卷积有效地较少了卷积的参数个数和计算量**。 

引入1x1卷积的作用：
- 对通道数进行升维和降维（跨通道信息整合），实现了多个特征图的线性组合，同时保持了原有的特征图大小；
- 相比于其他尺寸的卷积核，可以极大地降低运算复杂度；
- 如果使用两个3x3卷积堆叠，只有一个relu，但使用1x1卷积就会有两个relu，引入了更多的非线性映射；

我们来计算一下1x1卷积的计算量优势：首先看上图右边的Bottleneck结构，对于256维的输入特征，参数数目：1x1x256x64+3x3x64x64+1x1x64x256=69632，如果同样的输入输出维度但不使用1x1卷积，而使用两个3x3卷积的话，参数数目为(3x3x256x256)x2=1179648。简单计算下就知道了，使用了1x1卷积的bottleneck将计算量简化为原有的5.9%，收益超高。

```python

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d

        width = int(planes * (base_width / 64.)) * groups
        
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)

        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        # 通过一个1x1的卷积减少通道数，使得中间卷积的通道数减少为1/4
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        # 中间的普通卷积做完卷积后输出通道数等于输入通道数
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        # 第三个卷积用于增加（恢复）通道数，使得bottleneck的输出通道数等于bottleneck的输入通道数
        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        # 这个是关键，将上一层的输入，直接加到当前层的输出中，然后在做激活层
        out += identity
        out = self.relu(out)

        return out
```

___
### ResNet网络架构

不同数量的bottleneck组成了不同的resnet：

![](/img/cnn/resnet_bottleneck_3.PNG)

从上图可以看到，ResNet共有5组卷积（conv1，conv2_x...）。第一组卷积的输入大小是224x224，第五组卷积的输出大小是7x7，缩小了32（2^5）倍。每次缩小2倍，总共缩小5次，且每次都是在每组卷积的第一层上使stride为2。


**整个ResNet没有使用dropout，全部使用BN**。根据上图，我们不难发现一些规律和特点：

- 受VGG的启发，卷积层主要是3×3卷积；
- 对于相同的输出特征图大小的层，即同一个layer，具有相同数量的3x3滤波器;
- 如果特征地图大小减半，滤波器的数量加倍以保持每层的时间复杂度；（这句是论文和现场演讲中的原话，虽然我并不理解是什么意思）
- 每个layer通过步长为2的卷积层执行下采样，而且这个下采样只会在每一个layer的第一个Bottleneck的3x3卷积完成，有且仅有一次。
- 网络以平均池化层和softmax的1000路全连接层结束，实际上工程上一般用自适应全局平均池化 (Adaptive Global Average Pooling)；


第一组卷积：
```python
        # 第一组卷积的输入大小是224x224,使用64个 7*7卷积，输出大小 (224 + 2*3 - 7)/2 + 1 = 112
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # 缩小2倍
```
所有的ResNet网络输入部分是一个size=7x7, stride=2的大卷积核，以及一个size=3x3, stride=2的最大池化组成，通过这一步，一个224x224的输入图像先缩小为112x212,再缩小为56x56的特征图，极大减少了存储所需大小。

第二组-第五组卷积：

```python

        self.layer1 = self._make_layer(block, 64, layers[0])

        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,
                                       dilate=replace_stride_with_dilation[0])
        
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
                                       dilate=replace_stride_with_dilation[1])
         # 第五组卷积的输出大小是7x7
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
                                       dilate=replace_stride_with_dilation[2])

```
网络收尾部分：
```python
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

```

网络以平均池化层和softmax的1000路全连接层结束。

- 相比传统的分类网络，这里接的是池化，而不是全连接层。池化是不需要参数的，相比于全连接层可以砍去大量的参数。对于一个7x7的特征图，直接池化和改用全连接层相比，可以节省将近50倍的参数，作用有二：一是节省计算资源，二是防止模型过拟合，提升泛化能力；
- 这里使用的是全局平均池化，为什么不用最大池化呢？解释很多，查阅到的一些论文的实验结果表明平均池化的效果略好于最大池化，但最大池化的效果也差不到哪里去。实际使用过程中，可以根据自身需求做一些调整，比如多分类问题更适合使用全局最大池化。如果不确定话还有一个更保险的操作，就是最大池化和平均池化都做，然后把两个张量拼接，让后续的网络自己学习权重使用。


___
ResNet整体架构

```python

class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,
                 groups=1, width_per_group=64, replace_stride_with_dilation=None,
                 norm_layer=None):
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError("replace_stride_with_dilation should be None "
                             "or a 3-element tuple, got {}".format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group

        # 第一组卷积的输入大小是224x224,使用64个 7*7卷积，输出大小 (224 + 2*3 - 7)/2 + 1 = 112
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # 缩小2倍


        self.layer1 = self._make_layer(block, 64, layers[0]) # [3, 4, 23, 3]

        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,
                                       dilate=replace_stride_with_dilation[0])
        
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
                                       dilate=replace_stride_with_dilation[1])
        
         # 第五组卷积的输出大小是7x7
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
                                       dilate=replace_stride_with_dilation[2])
       
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)

    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1

        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        # 如果stride = 2， 在第一个Bottleneck的3x3卷积中，会使得feature map缩小一半
        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,
                            self.base_width, previous_dilation, norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups=self.groups,
                                base_width=self.base_width, dilation=self.dilation,
                                norm_layer=norm_layer))

        return nn.Sequential(*layers)

    def forward(self, x):
        # 输入
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        # 中间卷积
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # 输出
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x


````

### 从模型集成角度理解ResNet的有效性

[Residual Networks Behave Like Ensembles of Relatively Shallow Networks](http://papers.nips.cc/paper/6556-residual-networks-behave-like-ensembles-of-relatively-shallow-networks)论文指出，因为ResNet中存在着很多路径的集合，整个ResNet类似于多个网络的集成学习。删除部分ResNet的网络结点，不影响整个网络的性能，但是在VGG上做同样的事请网络立刻崩溃，由此可见相比其他网络ResNet对于部分路径的缺失不敏感。
![](/img/cnn/resnet_ensemble.PNG)


### 从梯度反向传播角度理解ResNet的有效性

残差结构使得梯度反向传播时，更不易出现梯度消失等问题，由于Skip Connection的存在，梯度能畅通无阻地通过各个Res blocks。因为梯度消失问题主要是发生在浅层，这种将深层梯度直接传递给浅层的做法，有效缓解了深度神经网络梯度消失的问题。


自定义ResNet

___
随后有一些论文对resnet的结构略微进行了修改（仍属于resnet结构），修改存在两方面：

- 修改bottleneck本身的结构
- 修改上面各组卷积层的数量搭配

### ResNet v2
何凯明在论文Identity Mappings in Deep Residual Networks中修改了残差块的结构，结构如下图：

![](/img/cnn/resnet_v2.PNG)

新结构将relu移到残差支路，有利于信号的前向传播和梯度的反向传播。虽然ResNet-V2似乎有些优点，但在各种任务中较少用来用作backbone网络。

### ResNeXt

主要是将残差块的中间的3x3卷积层变成group卷积，同时扩大了3x3卷积的输入输出通道数，使得在与对应的ResNet网络的计算量和参数个数相近的同时提高网络的性能。值得一提的是，ResNeXt与常用的ResNet对应的层数完全相同，都是50、101、152层。ResNeXt被很多网络用来当作backbone，例如Mask RCNN中除了使用ResNet，也尝试使用了ResNeXt-101和ResNeXt-152。



```python

import torch
import torch.nn as nn

__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',
           'wide_resnet50_2', 'wide_resnet101_2']


def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation)


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)





def _resnet(arch, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, **kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch],
                                              progress=progress)
        model.load_state_dict(state_dict)
    return model


def resnet18(pretrained=False, progress=True, **kwargs):

    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)


def resnet34(pretrained=False, progress=True, **kwargs):

    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet50(pretrained=False, progress=True, **kwargs):

    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet101(pretrained=False, progress=True, **kwargs):

    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)


def resnet152(pretrained=False, progress=True, **kwargs):
 
    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)


def resnext50_32x4d(pretrained=False, progress=True, **kwargs):

    kwargs['groups'] = 32
    kwargs['width_per_group'] = 4
    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def resnext101_32x8d(pretrained=False, progress=True, **kwargs):

    kwargs['groups'] = 32
    kwargs['width_per_group'] = 8
    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)


def wide_resnet50_2(pretrained=False, progress=True, **kwargs):

    kwargs['width_per_group'] = 64 * 2
    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def wide_resnet101_2(pretrained=False, progress=True, **kwargs):

    kwargs['width_per_group'] = 64 * 2
    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)

```



[ResNet 现场演讲](https://zhuanlan.zhihu.com/p/54072011)
[ResNet及其变种的结构梳理、有效性分析与代码解读](https://zhuanlan.zhihu.com/p/54289848)