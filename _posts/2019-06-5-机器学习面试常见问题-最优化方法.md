---
layout:     post
title:      机器学习面试常见问题 / 最优化方法
subtitle:   #副标题
date:       2019-6-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 面经
    - 机器学习
    - 正则项
    - L1范式
    - L2范式
---

注意区分训练误差和泛化误差，优化目标与深度学习目标，不一样；



优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差；

深度学习的目标在于降低泛化误差


本篇文章我们只关注优化算法在最小化目标函数上的表现，而不关注模型的泛化误差；


注意区分优化问题的**解析解**和**数值解**；


深度学习中绝大多数目标函数都很复杂。因此，很多优化问题并不存在解析解，而需要使用基于数值方法的优化算法找到近似解，即数值解；


由于深度学习模型参数通常都是高维的，目标函数的鞍点通常比局部最小值更常见；


### 梯度下降

在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）

### 随机梯度下降

随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。

### 小批量随机梯度下降


基于随机采样得到的梯度的方差在迭代过程中无法减小，因此在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减
梯度下降在迭代过程中一直使用目标函数的真实梯度，无须自我衰减学习率；

### 动量梯度下降法

**梯度下降的问题**

目标函数有关自变量的梯度代表了**目标函数在自变量当前位置**下降最快的方向。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。

[](/img/optimization/batch_gd.PNG)
[](/img/optimization/momentum_2.PNG)


可以看到，同一位置上，目标函数在竖直方向（ x2 轴方向）比在水平方向（ x1 轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大




### 



### 


### 


自适应池化(Adaptive Pooling)原理

若已知池化层的kernel_size、padding，stride以及输入张量的大小input_size，就可以得到输出张量大小output_size。

反之，给定输出张量大小，也能计算出对应的kernel_size