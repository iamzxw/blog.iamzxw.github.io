---
layout:     post   				    # 使用的布局（不需要改）
title:     	深度学习 / 分类模型 ResNeXt 论文笔记					# 标题 
subtitle:   CVPR2 017   #副标题
date:       2018-10-17 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - ResNeXt
---

[Aggregated Residual Transformations for Deep Neural Networks](http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf)

[Github](https://github.com/facebookresearch/ResNeXt)

[Github Pytorch](https://github.com/prlz77/ResNeXt.pytorch)

大白话：

- Inception 借鉴 ResNet，诞生了Inception-ResNet；
- ResNet 借鉴 Inception，诞生了ResNeXt：主要就是将ResNet的单路卷积变成多个支路的多路卷积，不过分组很多，结构一致，进行分组卷积。

作者一上来提出Inception的模式：split-transform-merge。

如下图所示，先将输入分配到多路，然后每一路进行转换，最后再把所有支路的结果融合。


![](/img/cnn/inception_block.PNG)

Inception 系列网络有个问题：网络的超参数设定的针对性比较强，当应用在别的数据集上时需要修改许多参数，因此可扩展性一般。


为了提高提高CNN分类模型的准确率，前人工作都是加深（VGG，ResNet）或加宽网络（Inception），但是随着超参数数量的增加（比如channels数，filter size等等），网络设计的难度和计算开销也会增加。


VGG，ResNet主要采用堆叠网络来实现。

Inception网络主要采用split-transform-merge策略


在这篇论文中提出网络 ResNeXt，同时采用 VGG 堆叠的思想和 Inception 的 split-transform-merge 思想，但是可扩展性比较强，

![](/img/cnn/resnext_block.PNG)

左边是ResNet的基本结构，右边是ResNeXt的基本结构：

作者进一步指出，split-transform-merge是通用的神经网络的标准范式，前面已经提到，基本的神经元符合这个范式，而如下图所示：

![](/img/cnn/resnext_block_2.PNG)

a是ResNeXt基本单元，如果把输出那里的1x1合并到一起，得到等价网络b拥有和Inception-ResNet相似的结构，而进一步把输入的1x1也合并到一起，得到等价网络c则和通道分组卷积的网络有相似的结构。

相当于在说，Inception-ResNet和通道分组卷积网络，都只是ResNeXt这一范式的特殊形式而已，

ResNeXt-50（32x4d），32指进入网络的第一个ResNeXt基本结构的分组数量C（即基数）为32，4d表示depth即每一个分组的通道数为4（所以第一个基本结构输入通道数为128）：


参数量不变，效果很好, 不过因为分组了多个分支单独进行处理，相较于原来整个一起卷积，硬件执行效率上会低一点，训练ResNeXt-101（32x4d）每个mini-batch要0.95s，而ResNet-101只要0.70s，

得益于精心设计的复杂的网络结构，ResNet-Inception v2可能效果会更好一点，但是ResNeXt的网络结构更简单，可以防止对于特定数据集的过拟合。而且更简单的网络意味着在用于自己的任务的时候，自定义和修改起来更简单。