---
layout:     post   				    # 使用的布局（不需要改）
title:     	2018 之江杯全球人工智能大赛 零样本图像识别 / 赛后总结
subtitle:   赛后总结   #副标题
date:       2018-11-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Zero-shot learning
    - 图像分类
    - 天池
---

___
### 背景
监督学习面临的问题
- 面对数以千万级别的数据时，人工标注图像标签成本过高；
- 对于新兴出现的事物，或者稀有事物，收集标注数据很困难，比如安全领域，敏感人物、色情、暴恐、药物等违规图像，本身数据量很少，突发性很强；

在只有少量标注数据，甚至没有标注数据的情况下，如何训练一个有效的模型。

传统的监督学习模型，如，各种CNN分类器，都是直接学习一个图像到标签的直接映射关系：给模型输入一张图像，直接输出对应的标签。  
零样本学习的做法是在图像和标签中间引入一个中间层，先将图像映射到中间层，然后再由中间层映射到标签。

___
### 1 如何获取有效的类别语义表示 ?

![](/img/zsl/semantic_embedding.PNG)

#### 1.1 从官方类别标签到WordNet类别标签
数据集中有一个类别打上了`cucumber`标签。从单词意思上看，这个类别应该是黄瓜。但是查看图片时发现，这个标签对应的图片都是海参的图片。海参英文名字叫`sea cucumber`，这就不奇怪了。这种情况说明，我们需要更为准确的类别名称。

另外，数据集里面的类别和`ImageNet`中的类别几乎完全重合，而`ImageNet`又是根据`WordNet`构建的。所以我们就把训练集里面的类别在`WordNet`中对应类别的id找出来。这样我们就可以不根据官方给出的类别标签获取词嵌入，而是根据WordNet中的类别名称获取词嵌入。

#### 1.2 从单个词嵌入到融合多个词嵌入
WordNet中每个节点对应一个物体类别，这个物体类别名称一般是多个单词组成。 比如，海参在WordNet中的名称为`sea cucumber, holothurian`
。在获取当前类别的语义向量时，根据WordNet中类别名称，分别获取类别名称中出现的单词的词向量，然后将所有词向量取平均。

#### 1.3 加入类别层级结构信息
更近一步。  
WordNet是一种树形的词典。可以根据当前类别节点的ID找到其父节点，比如，sea cucumber的父节点可能是海生动物、动物等等。将父节点的类别语义嵌入以一定权重加入，math.exp(-layer)。 


注：另外，我们还尝试了类别文本TF-IDF向量，使用Gensim在维基百科语料库上自行训练语言模型等，不过，效果都很差。


代码参考：

```python




```

___
### 2 如何训练CNN模型，获取具有辨别力的图片特征 ?

![](/img/zsl/training_cnn.PNG)

AB榜训练集中一共有365个类别，10万多张图片。


#### 2.1 数据增强 Data Augmentation
训练CNN模型，最常见的一种做法（trick）就是对数据集进行数据扩增：将原始图片进行水平翻转、剪切、缩放、轻微旋转等操作，这样可以从一个样本衍生出好几张样本。
直观感受，好像是通过这种方式增加了数据集的多样性和数量，从而更有效地训练模型。其实不然。我们事先知道图像经过上述操作后，图像里的物体还是那个物体。数据扩增是将我们的先验知识融入到模型当中的一种手段。

除了常规的数据增强，我们还采用两种比较有效数据增强方法：Mixup和随机擦除。

#### 随机擦除 Random Erasing




代码参考：

```python
class RandomErasing(object):
    def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=[0.5, 0.5, 0.5]):
        self.probability = probability
        self.mean = mean
        self.sl = sl
        self.sh = sh
        self.r1 = r1

    def __call__(self, img):

        if random.uniform(0, 1) > self.probability:
            return img

        for attempt in range(100):
            area = img.size()[1] * img.size()[2]

            target_area = random.uniform(self.sl, self.sh) * area
            aspect_ratio = random.uniform(self.r1, 1 / self.r1)

            h = int(round(math.sqrt(target_area * aspect_ratio)))
            w = int(round(math.sqrt(target_area / aspect_ratio)))

            if w < img.size()[2] and h < img.size()[1]:
                x1 = random.randint(0, img.size()[1] - h)
                y1 = random.randint(0, img.size()[2] - w)
                if img.size()[0] == 3:
                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]
                    img[1, x1:x1 + h, y1:y1 + w] = self.mean[1]
                    img[2, x1:x1 + h, y1:y1 + w] = self.mean[2]
                else:
                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]
                return img

        return img

```

#### Mixup
先说一下Sample Pairing。 [Sample Pairing](https://arxiv.org/abs/1801.02929)是IBM在2018年1月9日新发表的一篇论文。思想非常简单:从训练集中随机选取两张图片A、B，两者取平均得到一张新的混合图像C，从A、B图像对应的标签中随机选择一个，作为混合图像C的标签；然后将混合图像和对应的标签送入模型中训练。这样，可以使训练集规模从 N 扩增到 N.N。

[Mixup](https://arxiv.org/abs/1710.09412)是发表在 ICLR 2018 上的一篇论文。可以认为Mixup是Sample Pairing的升级版本：从[0,1]区间中随机采样一个值ε，输入图像A、B按照ε: 1 - ε比例叠加，输入标签也按照这个比例叠加。

虽然Sample Pairing或者Mixup都称为数据增强的一种方式，但是依照他们的做法：两张图像相加后，结果已经不是一幅合理的图像了，这跟我们通常说的数据增强完全不是一回事，为什么效果还会很好呢？

从正则项解读诠释。  

对于图像分类等任务, 我们希望找到一个模型f，使得y=f(x)，鉴于图像分类问题本身具有较强的非线性，所以我们一般会用非常深的网络来拟合。然而，网络越深也意味着更加容易对训练集过拟合。

假设模型已经有能力预测`y_a=f(x_a),y_b=f(x_b)`了，那么对于Mixup，它说这样还不够，模型还要同时对 `εx_a + (1 − ε)x_b` 输出 `εy_a + ( 1 − ε )y_b`才行，也就是`εy_a+( 1 − ε )y_b = f(εx_a + ( 1 − ε )x_b)`;  

将`y_a`,`y_b` 用`f(x_a)`,`f(x_b)`代替，那么得到:`εf(x_a) + ( 1 − ε )f(x_b) = f(εx_a + ( 1 − ε )x_b)`  

这其实是一个函数方程，假如`ε`,`xa`,`xb`都是任意的，那么上述函数方程的解f就是“线性函数”，也就是说，只有线性函数才能使得上式恒成立，换句话说，mixup希望模型f是一个线性函数。

mixup相当于一个正则项，它希望模型尽可能往线性函数靠近，也就是说，既保证模型预测尽可能准确，又让模型尽可能简单。在所有效果都差不多的模型中，选择最接近线性函数的那一个

代码参考：
```python

# 官方使用index = torch.randperm(batch_size)实现随机选择一个不同样本，这里直接调换，最终结果应该一样
def permute(x):
    pivot = x.size(0) // 2
    return torch.cat([x[pivot:], x[:pivot]], 0)

# 输入一个batch的数据(x, y)
def mixup(x, y, n_class, epsilon=0.1, alpha=1.):

    # one-hot形式
    y = torch.zeros([len(y), n_class], device=y.device).scatter_(1, y.view(-1, 1), 1)
    # 软标签
    y = (1 - epsilon) * y + epsilon / n_class

    # beta分布
    lam = np.random.beta(alpha, alpha)

    mix_x = x * lam + permute(x) * (1 - lam)
    mix_y = y * lam + permute(y) * (1 - lam)

    x = torch.cat([x, mix_x], 0)
    y = torch.cat([y, mix_y], 0)

    return x, y
```

> 另外，多说一句，在做机器学习任务时，会有很多绝妙的处理技巧，使得学习器性能表现的更好，但是这些绝妙的技巧都是人类的先验知识。加入的人类先验知识越多，说明机器学习非常依赖人工的处理，而没有达到智能。



___
### 3 如何做视觉空间与语义空间的映射关系 ?


___
### 4 如何做最终的类别预测模型？



对于训练集图像A，从训练集中再随机抓取图像B，将两张图像相加除以2（也就是求两者平均），但是标签仍然采用A的标签，然后将处理后的图像送入模型中训练。这样，可以使训练集规模从 N 扩增到 N.N。 有两点比较违反直觉的地方

- 图像A、B平均后，还是图像吗？
- 就算是图像，为什么A、B两者平均后，图像标签是A的标签而不是B的标签。