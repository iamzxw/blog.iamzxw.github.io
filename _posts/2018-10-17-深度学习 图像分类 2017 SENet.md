---
layout:     post   				    # 使用的布局（不需要改）
title:     	深度学习 / 分类模型 SENet 论文笔记				# 标题 
subtitle:      #副标题
date:       2018-10-17 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - SENet
    - 
---

[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

[Github caffe](https://github.com/hujie-frank/SENet)

[Github Pytorch](https://github.com/miraclewkf/SENet-PyTorch)


论文作者将SE Block与ResNeXt结合，组成SE-ResNeXt-152 (64 × 4d）,在ILSVRC 2017的分类比赛中取得冠军(其实是多模型融合)。




论文主要研究特征图之间的关系channels-wise。 网络中会根据卷积核抽取出许多特征图，让网络根据loss自动学习特征权重，使得有效的feature map权重大，无效或效果小的feature map权重小。
> In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels


___
### SE Block
![](/img/cnn/se_block.PNG)

输入X经过卷积核变换后，会生成特征图U，传统网络，会直接将特征图U作为下一层网络的输入；
而SE Block假定特征图U不是最优的，每个通道的重要程度不同，有的通道更有用，有的通道则不太有用。

对于特征图U的每个通道HxW，先global average pool，每个通道得到1个标量，C个通道得到C个数，然后经过FC-ReLU-FC-Sigmoid得到C个0到1之间的标量，作为通道的权重，将权重加权到原先的特征图U上，得到新的加权后的特征图，作者称之为feature recalibration。

第一步每个通道HxW个数全局平均池化得到一个标量，称之为Squeeze;

然后两个FC得到01之间的一个权重值，对原始的每个HxW的每个元素乘以对应通道的权重，得到新的feature map，称之为Excitation。


```python
class SEModule(nn.Module):

    def __init__(self, channels, reduction):
        super(SEModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,
                             padding=0)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,
                             padding=0)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        module_input = x
        x = self.avg_pool(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return module_input * x

```

SENet一个很大的优点就是可以很方便地集成到现有网络中，提升网络性能，并且代价很小。

### SE ResNet Block
![](/img/cnn/se_resnet_block.PNG)



### SE Inception Block
![](/img/cnn/se_inception_block.PNG)



提升很大，并且代价很小，通过对通道进行加权，强调有效信息，抑制无效信息，并且是一个通用方法，应用在Inception、 Inception-ResNet、 ResNet、ResNeXt都能有所提升，适用范围很广。

思路很清晰简洁，实现很简单，用起来很方便，各种实验都证明了其有效性，各种任务都可以尝试一下，效果应该不会太差。