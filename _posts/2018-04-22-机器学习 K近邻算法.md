---
layout:     post   				    # 使用的布局（不需要改）
title:      机器学习 / K近邻算法 				# 标题 
subtitle:      #副标题
date:       2018-04-22 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 机器学习
    - 分类算法
    - 懒惰学习
---

K近邻算法是一种相对来说比较简单的分类算法。

思想比较直观：给定一个训练数据集，对于新的测试实例，在训练数据集中找到与该测试实例最邻近的k个实例，这k个实例的多数属于某个类，就把该测试实例分为这个类，没有显示参数训练过程。理想情况下（密采样），最近邻分类器的错误率不超过贝叶斯最优分类器错误率的两倍。

这里面有涉及到三个比较关键的因素：
- k值的选择；
- 距离度量：采用哪种距离度量来表达最近邻的关系；
- 最终的决策规则：把这k个最近邻实例找到后，如何做分类决策；

### k值的选择

当k值较小时，整体模型变得复杂，容易发生过拟合；非常符合训练集分布规律，对邻近的实例点非常敏感；
当k值较大时，整体模型变得比较简单，极端情况下当k等于训练集个数时，无论输入实例x是什么，都将简单的预测它属于在训练集中实例最多的类别；

### 距离度量

输入特征向量一般都是高维度的。在高维空间中，有数据样本稀疏、距离计算困难等问题，所以采用哪种距离度量很关键。

常用的有闵可夫斯基距离，余弦距离。

> 欧式距离：
np.sqrt(np.sum(np.square(v1 - v2)))
np.linalg.norm(v1 - v2, ord=2)

曼哈顿距离：
np.sum(np.abs(v1 - v2))
np.linalg.norm(v1 -v2, ord=1)

切比雪夫距离：
np.abs(v1 - v2).max()
np.linalg.norm(v1-v2, ord=np.inf)

余弦距离：
np.dot(v1, v2) / np.linalg.norm(v1, ord=2)*np.linalg.norm(v2, ord=2)

汉明距离：两个等长字符串s1和s2之间的汉明距离定义为将其中一个变为另一个所需最小替换次数。

杰卡德相似系数：AB交集元素在AB并集元素中所占的比例
杰卡德距离：


### 决策规则

按照某种距离度量选出k个最相邻的样本点后，对于分类问题，决策规则最简单的就是投票法，k个最近邻样本点中，多数属于某个类，就把测试实例分为该类别。


Sklearn.neighbors.DistanceMetirc.get(‘’)  ## Euclidean, manhattan,chebyshev,minkowski




> 代码实现：

import numpy as np
import math

def KNNClassify(newInput, dataSet, labels, k):

## step 1 compute all distance
distance = [0]*dataSet.shape[0]
for i in range(dataSet.shape[0]):
distance[i] = consice_distance(newInput, dataSet[i])

## step 2 sorted distance
sotedDistIndices = np.argsort(distance)

## step 3 compute top k distance point label’s count
classCount = {} #{label: count}
For i in range(k):
K_label = labels[sortedDistIndices[i]]
classCount[k_label] = classCount.get(k_label, 0) + 1

## step 4 select max voted class
maxCount= 0
For key, value in classCount.iterms():
If value > maxClass:
maxCount = value
maxClass = key

Return maxClass