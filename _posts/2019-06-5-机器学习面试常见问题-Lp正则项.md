---
layout:     post
title:      机器学习面试常见问题 / Lp正则项
subtitle:   #副标题
date:       2019-6-5 				# 时间
author:     zhu.xinwei 		    	# 作者
header-img: img/post-bg-desk.jpg	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 面经
    - 机器学习
    - 正则项
    - L1范式
    - L2范式
---

简单来说，监督学习问题就是“minimize your error while regularizing your parameters ”：最小化误差的同时，正则化参数。

最小化误差是为了让模型拟合训练数据，而正则化参数是防止模型过度拟合训练数据。

所以，监督学习的目标函数一般是： Loss函数 + 正则项；
Square Loss ： 最小二乘
Hinge Loss： SVM
Exp Loss: Boosting
Log Loss: 逻辑回归

不同的Loss函数，具有不同的拟合特性，所以对应不同的学习任务。我们先不深究Loss函数的问题，下面主要来看一下正则项的特性。

正则项的使用还可以约束模型的特性，这样就可以将人对模型的先验知识融入到模型学习当中，强制让学习到的模型具有人们想要的特性。比如，稀疏、底秩、平滑等。

正则项符合奥卡姆剃刀(Occam's razor)原理：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型
从贝叶斯估计的角度来看，正则项对应于模型的先验概率。


正则项函数也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，正则项值就越大。优化目标函数，就是为了降低模型复杂度（奥卡姆剃刀原理），防止过拟合。

比如，正则项可以是模型参数的Lp范式，不同的约束范式，取得的效果也不同。
- L0范式
- L1范式
- 迹范式
- Frobenius弗罗贝尼乌斯范式
- 核范式


___
### 什么是L1正则、L2正则？

L1正则: 将网络参数的绝对值和加入到目标函数中，在优化目标函数时，使得参数变得稀疏


L2正则: 将网络参数的平方和加入到目标函数中，在优化目标函数时，使得参数变得平滑


### L1正则项、L2正则项两者的区别是什么？

- 计算方式不同：L1正则是将网络参数的绝对值和加入到目标函数，L2正则是将网络参数的平方和加入到目标函数；
- 最终效果不同：L1正则使得网络参数变得稀疏，L2正则使得网络参数变得平滑

### L1正则为什么可以得到稀疏解？


### 为什么L2正则能提升模型的泛化能力？


### 为什么说”L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则项相当于进入了高斯先验“？